---
layout: page
breadcrumb: true

subheadline:  "Lecture: February 2, 2021"
title:  "What is PCA?"
teaser: ""
categories:
    - blog
tags:
    - complexity
    - math
    - dimensionality reduction
    - impact
header:
    image_fullwidth: "header_8.jpg"
permalink: "/blog/pca/"
---


# Matrices as linear transformations

## Eigenvectors are vectors that are only multiplied by a constant under the matrix transformation

## Eigenvalues describe how much each eigenvector is stretched by the matrix transformation

## Eigenvectors describe patterns ..... [TODO]

# The covariance matrix describes relationships between features

## Why do we even care about covariance?

# PCA as a transformation of the covariance matrix

## Eigenvectors of the covariance matrix describe biggest relationships (highest covariance) between features

## Eigenvalues describe the magnitude of those relationships

## Eigenvectors ordered by eigenvalue magnitude give principle components

## First principle component is the direction of highest variation in the data

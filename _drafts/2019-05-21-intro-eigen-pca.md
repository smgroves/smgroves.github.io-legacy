---
  layout: post
  title: "Introduction to Eigenvalues and PCA"
---

# Matrices as linear transformations

## Eigenvectors are vectors that are only multiplied by a constant under the matrix transformation

## Eigenvalues describe how much each eigenvector is stretched by the matrix transformation

## Eigenvectors describe patterns ..... [TODO]

# The covariance matrix describes relationships between features

## Why do we even care about covariance?

# PCA as a transformation of the covariance matrix

## Eigenvectors of the covariance matrix describe biggest relationships (highest covariance) between features

## Eigenvalues describe the magnitude of those relationships

## Eigenvectors ordered by eigenvalue magnitude give principle components

## First principle component is the direction of highest variation in the data
